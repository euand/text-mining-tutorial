{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook introduces how to use the topicmodels module for implementing Latent Dirichlet Allocation using the collapsed Gibbs sampling algorithm of Griffiths and Steyvers (2004).  The module contains three classes: one for processing raw text, another for implementing LDA, and another for querying.  This tutorial will go through the main features of each, for full details see the documented source code.\n",
    "\n",
    "To illustrate LDA, the tutorial uses text data from State of the Union Addresses at the paragraph level.  These are available for download from http://www.presidency.ucsb.edu/sou.php.  They are contained in the tab-separated text file speech_data_extend.txt distributed with this tutorial.\n",
    "\n",
    "To interact with this data, we begin by importing some libraries that are not strictly speaking necessary for using topicmodels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, we read in the data, specifying the encoding of the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_table(data_dir + \"speech_data_extend.txt\",encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data object is called a pandas DataFrame, and is similar to a Data Frame in R.  One can see the data has three fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'president', u'speech', u'year'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the tutorial we focus on State of the Union addresses made since the television era, which began in 1947."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9488"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[data.year >= 1947]\n",
    "len(data) # The number of documents (paragraphs of State of the Union Addresses) in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Raw Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now import topicmodels, the module used in most of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import topicmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before implementing a topic model, it is important to pre-process the data.  The first class in the topicmodels module is called RawDocs and facilitates this pre-processing.  We are going to pass this class the text data contained in the DataFrame along with a list of stopwords specified by \"long\".  Stopwords are common words in English that tend to appear in all text, and so are not helpful in describing content.  There is no definitive list of stopwords, and another option we describe below is to let the data itself reveal which words are useful for discriminating among documents.  The list of words comes from http://snowball.tartarus.org/algorithms/english/stop.txt, but one need not use all of them.  In Hansen, McMahon, and Prat (2014), for example, we use just a subset of these, which you can use by specifying \"short\" instead of \"long\".  (You can view the stopwords by typing docsobj.stopwords into the interpreter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsobj = topicmodels.RawDocs(data.speech, \"long\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than passing the text as a DataFrame column, one can also pass a text file, in which case each new line will be read as a separate document.\n",
    "\n",
    "docsobj is now an object with several attributes.  The most important is its tokens attribute.  This is the outcome of taking each raw document, converting each contraction into its underlying words (e.g. \"don't\" into \"do not\"), coverting it into lowercase, and breaking it into its underlying linguistic elements (words, numbers, punctuation, etc.).  To illustrate, compare the fourth paragraph in the 1947 State of the Union Address as a raw document and after tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I come also to welcome you as you take up your duties and to discuss with you the manner in which you and I should fulfill our obligations to the American people during the next 2 years. \n",
      "[u'i', u'come', u'also', u'to', u'welcome', u'you', u'as', u'you', u'take', u'up', u'your', u'duties', u'and', u'to', u'discuss', u'with', u'you', u'the', u'manner', u'in', u'which', u'you', u'and', u'i', u'should', u'fulfill', u'our', u'obligations', u'to', u'the', u'american', u'people', u'during', u'the', u'next', u'2', u'years', u'.']\n"
     ]
    }
   ],
   "source": [
    "print data.speech.values[3] # fourth paragraph (note that Python uses 0-indexing)\n",
    "print docsobj.tokens[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, all tokens are in the dataset, which can be useful for some purposes.  For example, one might want to count the number of question marks in each speech.  For implementing LDA, though, one generally wishes to focus on words.  docsobj has a method to clean tokens.  This will remove all non-alphanumeric tokens, and, by default, all numeric tokens as well (to keep numeric tokens, pass False as the second argument in parentheses).  The number passed as an argument removes all tokens whose length is less that - this can be useful if some symbols in the data like copyright signs trascribed as a single 'c' that the user would like to remove, in which case one would pass 1.  In this case, we pass 1 for illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsobj.token_clean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'come', u'also', u'to', u'welcome', u'you', u'as', u'you', u'take', u'up', u'your', u'duties', u'and', u'to', u'discuss', u'with', u'you', u'the', u'manner', u'in', u'which', u'you', u'and', u'should', u'fulfill', u'our', u'obligations', u'to', u'the', u'american', u'people', u'during', u'the', u'next', u'years']\n"
     ]
    }
   ],
   "source": [
    "print docsobj.tokens[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we remove the stopwords we passed when creating docsobj.  Here we pass \"tokens\" as an argument to specify that the stopwords should be removed from docsobj.tokens.  The other option would be to pass \"stems\", which we discuss below.  Notice by how much we have reduced the size of the document by removing stopwords!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'come', u'welcome', u'duties', u'discuss', u'manner', u'fulfill', u'obligations', u'american', u'people', u'next', u'years']\n"
     ]
    }
   ],
   "source": [
    "docsobj.stopword_remove(\"tokens\")\n",
    "print docsobj.tokens[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to attempt to group together words that are grammatically different but themeatically identical.  For example, the document above has the token \"obligations\" but another may have \"obligation\" and yet another \"oblige.\"  Ultimately these three words denote the same concept, and so we might want them to share the same symbol.  One way to achieve this is through stemming, a process whereby words are transformed through a deterministic algorithm to a base form.  One popular stemmer is the Porter stemmer, which docsobj applies (via its implementation in Python's Natural Language Toolkit).  This creates a new stems attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'come', u'welcom', u'duti', u'discuss', u'manner', u'fulfil', u'oblig', u'american', u'peopl', u'next', u'year']\n"
     ]
    }
   ],
   "source": [
    "docsobj.stem()\n",
    "print docsobj.stems[3]\n",
    "docsobj.stopword_remove(\"stems\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the outcome of stemming need not be an English word.  These stems are the data on which we will run the topic model below.  We make an additional call to remove stopwords from stems, since the stemmed forms of tokens not in the stopword list may themselves be in the stopword list.\n",
    "\n",
    "The final step in pre-processing is to drop remaining words that are not useful for identifying content.  We have already dropped standard stopwords, but there may also be data-dependent common words.  For example, in data from Supreme Court proceedings, \"justice\" might be treated as a stopword.  Also, words that appear just once or twice in the collection are not informative of content either.  Ideally, one would like a measure of informativeness that both punishes common words in the data, and rare words.  One such option is to give each stem a tf-idf (term frequency - inverse document frequency) score.  This is standard in the language processing literature, so we omit details here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsobj.term_rank(\"stems\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This call produces two .csv files in the working directory, df_ranking.csv and tfidf_ranking.csv.  df_ranking.csv ranks each stem according to its document frequency, or the number of documents it appears in.  tfidf_ranking.csv ranks each stem accroding to the tf-idf measure, according to which highly informative words are those that appear frequently in the entire dataset, but in relatively few documents.  Stems with the highest scores include \"gun\", \"iraq\", and \"immigr\".\n",
    "\n",
    "At this stage, the user can decide how many stems to drop based on either the df or tf-idf scores.  The first argument to rank_remove specifies the ranking method to use (\"df\" or \"tfidf\"), the second whether to drop from \"tokens\" or \"stems\" (since we formed the rankings based on stems above, we should specify stems), and finally the value to use for the cutoff for dropping stems.  One might instead prefer to provide a number $n$ such that all stems with a tf-idf value less than or equal to the $n$th ranked stem are then dropped, which we illustrate below.  One can determine the cutoff from exploring the csv files, but here we plot the ranking in Python, which indicates a reasonable cutoff is 5000.  (When using df rather than tfidf, substitute docsobj.df_ranking)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdf36168710>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdf399d42d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([x[1] for x in docsobj.tfidf_ranking])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique stems = 4742\n",
      "number of total stems = 250000\n"
     ]
    }
   ],
   "source": [
    "docsobj.rank_remove(\"tfidf\",\"stems\",docsobj.tfidf_ranking[5000][1])\n",
    "all_stems = [s for d in docsobj.stems for s in d]\n",
    "print(\"number of unique stems = %d\" % len(set(all_stems)))\n",
    "print(\"number of total stems = %d\" % len(all_stems))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After pre-processing, we have 4742 unique stems, and 250000 total stems.  We now proceed to estimate a topic model on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating a Topic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in estimation is to initialize a model via topicmodels' LDA class.  We will pass docsobj.stems as the set of documents, and we also need to decide on a number of topics.  Here we choose 30 topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldaobj = topicmodels.LDA.LDAGibbs(docsobj.stems,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three main parameters in LDA, the number of topics, and the two hyperparameters of the Dirichlet priors.  topicmodels.LDA follows the advice of Griffiths and Steyvers (2004) and sets the hyperparameter of the Dirichlet prior on topics to $200/V$, where $V$ is the number of unique vocabulary elements, and the hyperparameter of the Dirichlet prior on document-topic distributions to $50/K$, where $K$ is the number of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "1.66666666667\n",
      "0.0421762969211\n"
     ]
    }
   ],
   "source": [
    "print ldaobj.K # number of topic, user defined.\n",
    "print ldaobj.alpha # hyperparameter for document-topic distribution, automatically defined\n",
    "print ldaobj.beta # hyperparameter for topics, automatically defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should users wish to define their own priors, they can do so by calling ldaobj.set_prior(alpha,beta).  \n",
    "\n",
    "Another quantity set automatically by topicmodels.LDA is a random allocation of stems to topic assignments.  It is a 250000-dimensional vector of integers in $\\{0,\\ldots,29\\}$.  Should the user wish to define another seed, call ldaobj.set_seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20 15 28 13 11 11 18  0 23  1]\n",
      "(250000,)\n"
     ]
    }
   ],
   "source": [
    "print ldaobj.topic_seed[:10]\n",
    "print ldaobj.topic_seed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have initialized our topic model, we are ready to sample.  To sample, we pass three parameters.  The first is the number of iterations we want the chain to burn in before beginning to sample.  The second is a thinning interval, the number of iterations to let the chain run between samples.  Allowing for a thinning interval reduces autocorrelation between samples.  The third is the number of samples to take.  So, for example, if the user passes (1000,50,20) the following will happen.  First, the chain will run for 1,000 iterations.  Then 20 samples will be taken corresponding to the $\\{1050,1100\\ldots,1950,2000\\}$ iterations for a total of 2000 iterations overall.\n",
    "\n",
    "In order not to waste time in the tutorial, we start with a relatively short chain with no burnin, a thinning interval of 50, and 10 samples, for a total of 500 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 of (collapsed) Gibbs sampling\n",
      "Iteration 2 of (collapsed) Gibbs sampling\n",
      "Iteration 3 of (collapsed) Gibbs sampling\n",
      "Iteration 4 of (collapsed) Gibbs sampling\n",
      "Iteration 5 of (collapsed) Gibbs sampling\n",
      "Iteration 6 of (collapsed) Gibbs sampling\n",
      "Iteration 7 of (collapsed) Gibbs sampling\n",
      "Iteration 8 of (collapsed) Gibbs sampling\n",
      "Iteration 9 of (collapsed) Gibbs sampling\n",
      "Iteration 10 of (collapsed) Gibbs sampling\n",
      "Iteration 11 of (collapsed) Gibbs sampling\n",
      "Iteration 12 of (collapsed) Gibbs sampling\n",
      "Iteration 13 of (collapsed) Gibbs sampling\n",
      "Iteration 14 of (collapsed) Gibbs sampling\n",
      "Iteration 15 of (collapsed) Gibbs sampling\n",
      "Iteration 16 of (collapsed) Gibbs sampling\n",
      "Iteration 17 of (collapsed) Gibbs sampling\n",
      "Iteration 18 of (collapsed) Gibbs sampling\n",
      "Iteration 19 of (collapsed) Gibbs sampling\n",
      "Iteration 20 of (collapsed) Gibbs sampling\n",
      "Iteration 21 of (collapsed) Gibbs sampling\n",
      "Iteration 22 of (collapsed) Gibbs sampling\n",
      "Iteration 23 of (collapsed) Gibbs sampling\n",
      "Iteration 24 of (collapsed) Gibbs sampling\n",
      "Iteration 25 of (collapsed) Gibbs sampling\n",
      "Iteration 26 of (collapsed) Gibbs sampling\n",
      "Iteration 27 of (collapsed) Gibbs sampling\n",
      "Iteration 28 of (collapsed) Gibbs sampling\n",
      "Iteration 29 of (collapsed) Gibbs sampling\n",
      "Iteration 30 of (collapsed) Gibbs sampling\n",
      "Iteration 31 of (collapsed) Gibbs sampling\n",
      "Iteration 32 of (collapsed) Gibbs sampling\n",
      "Iteration 33 of (collapsed) Gibbs sampling\n",
      "Iteration 34 of (collapsed) Gibbs sampling\n",
      "Iteration 35 of (collapsed) Gibbs sampling\n",
      "Iteration 36 of (collapsed) Gibbs sampling\n",
      "Iteration 37 of (collapsed) Gibbs sampling\n",
      "Iteration 38 of (collapsed) Gibbs sampling\n",
      "Iteration 39 of (collapsed) Gibbs sampling\n",
      "Iteration 40 of (collapsed) Gibbs sampling\n",
      "Iteration 41 of (collapsed) Gibbs sampling\n",
      "Iteration 42 of (collapsed) Gibbs sampling\n",
      "Iteration 43 of (collapsed) Gibbs sampling\n",
      "Iteration 44 of (collapsed) Gibbs sampling\n",
      "Iteration 45 of (collapsed) Gibbs sampling\n",
      "Iteration 46 of (collapsed) Gibbs sampling\n",
      "Iteration 47 of (collapsed) Gibbs sampling\n",
      "Iteration 48 of (collapsed) Gibbs sampling\n",
      "Iteration 49 of (collapsed) Gibbs sampling\n",
      "Iteration 50 of (collapsed) Gibbs sampling\n",
      "Iteration 51 of (collapsed) Gibbs sampling\n",
      "Iteration 52 of (collapsed) Gibbs sampling\n",
      "Iteration 53 of (collapsed) Gibbs sampling\n",
      "Iteration 54 of (collapsed) Gibbs sampling\n",
      "Iteration 55 of (collapsed) Gibbs sampling\n",
      "Iteration 56 of (collapsed) Gibbs sampling\n",
      "Iteration 57 of (collapsed) Gibbs sampling\n",
      "Iteration 58 of (collapsed) Gibbs sampling\n",
      "Iteration 59 of (collapsed) Gibbs sampling\n",
      "Iteration 60 of (collapsed) Gibbs sampling\n",
      "Iteration 61 of (collapsed) Gibbs sampling\n",
      "Iteration 62 of (collapsed) Gibbs sampling\n",
      "Iteration 63 of (collapsed) Gibbs sampling\n",
      "Iteration 64 of (collapsed) Gibbs sampling\n",
      "Iteration 65 of (collapsed) Gibbs sampling\n",
      "Iteration 66 of (collapsed) Gibbs sampling\n",
      "Iteration 67 of (collapsed) Gibbs sampling\n",
      "Iteration 68 of (collapsed) Gibbs sampling\n",
      "Iteration 69 of (collapsed) Gibbs sampling\n",
      "Iteration 70 of (collapsed) Gibbs sampling\n",
      "Iteration 71 of (collapsed) Gibbs sampling\n",
      "Iteration 72 of (collapsed) Gibbs sampling\n",
      "Iteration 73 of (collapsed) Gibbs sampling\n",
      "Iteration 74 of (collapsed) Gibbs sampling\n",
      "Iteration 75 of (collapsed) Gibbs sampling\n",
      "Iteration 76 of (collapsed) Gibbs sampling\n",
      "Iteration 77 of (collapsed) Gibbs sampling\n",
      "Iteration 78 of (collapsed) Gibbs sampling\n",
      "Iteration 79 of (collapsed) Gibbs sampling\n",
      "Iteration 80 of (collapsed) Gibbs sampling\n",
      "Iteration 81 of (collapsed) Gibbs sampling\n",
      "Iteration 82 of (collapsed) Gibbs sampling\n",
      "Iteration 83 of (collapsed) Gibbs sampling\n",
      "Iteration 84 of (collapsed) Gibbs sampling\n",
      "Iteration 85 of (collapsed) Gibbs sampling\n",
      "Iteration 86 of (collapsed) Gibbs sampling\n",
      "Iteration 87 of (collapsed) Gibbs sampling\n",
      "Iteration 88 of (collapsed) Gibbs sampling\n",
      "Iteration 89 of (collapsed) Gibbs sampling\n",
      "Iteration 90 of (collapsed) Gibbs sampling\n",
      "Iteration 91 of (collapsed) Gibbs sampling\n",
      "Iteration 92 of (collapsed) Gibbs sampling\n",
      "Iteration 93 of (collapsed) Gibbs sampling\n",
      "Iteration 94 of (collapsed) Gibbs sampling\n",
      "Iteration 95 of (collapsed) Gibbs sampling\n",
      "Iteration 96 of (collapsed) Gibbs sampling\n",
      "Iteration 97 of (collapsed) Gibbs sampling\n",
      "Iteration 98 of (collapsed) Gibbs sampling\n",
      "Iteration 99 of (collapsed) Gibbs sampling\n",
      "Iteration 100 of (collapsed) Gibbs sampling\n",
      "Iteration 101 of (collapsed) Gibbs sampling\n",
      "Iteration 102 of (collapsed) Gibbs sampling\n",
      "Iteration 103 of (collapsed) Gibbs sampling\n",
      "Iteration 104 of (collapsed) Gibbs sampling\n",
      "Iteration 105 of (collapsed) Gibbs sampling\n",
      "Iteration 106 of (collapsed) Gibbs sampling\n",
      "Iteration 107 of (collapsed) Gibbs sampling\n",
      "Iteration 108 of (collapsed) Gibbs sampling\n",
      "Iteration 109 of (collapsed) Gibbs sampling\n",
      "Iteration 110 of (collapsed) Gibbs sampling\n",
      "Iteration 111 of (collapsed) Gibbs sampling\n",
      "Iteration 112 of (collapsed) Gibbs sampling\n",
      "Iteration 113 of (collapsed) Gibbs sampling\n",
      "Iteration 114 of (collapsed) Gibbs sampling\n",
      "Iteration 115 of (collapsed) Gibbs sampling\n",
      "Iteration 116 of (collapsed) Gibbs sampling\n",
      "Iteration 117 of (collapsed) Gibbs sampling\n",
      "Iteration 118 of (collapsed) Gibbs sampling\n",
      "Iteration 119 of (collapsed) Gibbs sampling\n",
      "Iteration 120 of (collapsed) Gibbs sampling\n",
      "Iteration 121 of (collapsed) Gibbs sampling\n",
      "Iteration 122 of (collapsed) Gibbs sampling\n",
      "Iteration 123 of (collapsed) Gibbs sampling\n",
      "Iteration 124 of (collapsed) Gibbs sampling\n",
      "Iteration 125 of (collapsed) Gibbs sampling\n",
      "Iteration 126 of (collapsed) Gibbs sampling\n",
      "Iteration 127 of (collapsed) Gibbs sampling\n",
      "Iteration 128 of (collapsed) Gibbs sampling\n",
      "Iteration 129 of (collapsed) Gibbs sampling\n",
      "Iteration 130 of (collapsed) Gibbs sampling\n",
      "Iteration 131 of (collapsed) Gibbs sampling\n",
      "Iteration 132 of (collapsed) Gibbs sampling\n",
      "Iteration 133 of (collapsed) Gibbs sampling\n",
      "Iteration 134 of (collapsed) Gibbs sampling\n",
      "Iteration 135 of (collapsed) Gibbs sampling\n",
      "Iteration 136 of (collapsed) Gibbs sampling\n",
      "Iteration 137 of (collapsed) Gibbs sampling\n",
      "Iteration 138 of (collapsed) Gibbs sampling\n",
      "Iteration 139 of (collapsed) Gibbs sampling\n",
      "Iteration 140 of (collapsed) Gibbs sampling\n",
      "Iteration 141 of (collapsed) Gibbs sampling\n",
      "Iteration 142 of (collapsed) Gibbs sampling\n",
      "Iteration 143 of (collapsed) Gibbs sampling\n",
      "Iteration 144 of (collapsed) Gibbs sampling\n",
      "Iteration 145 of (collapsed) Gibbs sampling\n",
      "Iteration 146 of (collapsed) Gibbs sampling\n",
      "Iteration 147 of (collapsed) Gibbs sampling\n",
      "Iteration 148 of (collapsed) Gibbs sampling\n",
      "Iteration 149 of (collapsed) Gibbs sampling\n",
      "Iteration 150 of (collapsed) Gibbs sampling\n",
      "Iteration 151 of (collapsed) Gibbs sampling\n",
      "Iteration 152 of (collapsed) Gibbs sampling\n",
      "Iteration 153 of (collapsed) Gibbs sampling\n",
      "Iteration 154 of (collapsed) Gibbs sampling\n",
      "Iteration 155 of (collapsed) Gibbs sampling\n",
      "Iteration 156 of (collapsed) Gibbs sampling\n",
      "Iteration 157 of (collapsed) Gibbs sampling\n",
      "Iteration 158 of (collapsed) Gibbs sampling\n",
      "Iteration 159 of (collapsed) Gibbs sampling\n",
      "Iteration 160 of (collapsed) Gibbs sampling\n",
      "Iteration 161 of (collapsed) Gibbs sampling\n",
      "Iteration 162 of (collapsed) Gibbs sampling\n",
      "Iteration 163 of (collapsed) Gibbs sampling\n",
      "Iteration 164 of (collapsed) Gibbs sampling\n",
      "Iteration 165 of (collapsed) Gibbs sampling\n",
      "Iteration 166 of (collapsed) Gibbs sampling\n",
      "Iteration 167 of (collapsed) Gibbs sampling\n",
      "Iteration 168 of (collapsed) Gibbs sampling\n",
      "Iteration 169 of (collapsed) Gibbs sampling\n",
      "Iteration 170 of (collapsed) Gibbs sampling\n",
      "Iteration 171 of (collapsed) Gibbs sampling\n",
      "Iteration 172 of (collapsed) Gibbs sampling\n",
      "Iteration 173 of (collapsed) Gibbs sampling\n",
      "Iteration 174 of (collapsed) Gibbs sampling\n",
      "Iteration 175 of (collapsed) Gibbs sampling\n",
      "Iteration 176 of (collapsed) Gibbs sampling\n",
      "Iteration 177 of (collapsed) Gibbs sampling\n",
      "Iteration 178 of (collapsed) Gibbs sampling\n",
      "Iteration 179 of (collapsed) Gibbs sampling\n",
      "Iteration 180 of (collapsed) Gibbs sampling\n",
      "Iteration 181 of (collapsed) Gibbs sampling\n",
      "Iteration 182 of (collapsed) Gibbs sampling\n",
      "Iteration 183 of (collapsed) Gibbs sampling\n",
      "Iteration 184 of (collapsed) Gibbs sampling\n",
      "Iteration 185 of (collapsed) Gibbs sampling\n",
      "Iteration 186 of (collapsed) Gibbs sampling\n",
      "Iteration 187 of (collapsed) Gibbs sampling\n",
      "Iteration 188 of (collapsed) Gibbs sampling\n",
      "Iteration 189 of (collapsed) Gibbs sampling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 190 of (collapsed) Gibbs sampling\n",
      "Iteration 191 of (collapsed) Gibbs sampling\n",
      "Iteration 192 of (collapsed) Gibbs sampling\n",
      "Iteration 193 of (collapsed) Gibbs sampling\n",
      "Iteration 194 of (collapsed) Gibbs sampling\n",
      "Iteration 195 of (collapsed) Gibbs sampling\n",
      "Iteration 196 of (collapsed) Gibbs sampling\n",
      "Iteration 197 of (collapsed) Gibbs sampling\n",
      "Iteration 198 of (collapsed) Gibbs sampling\n",
      "Iteration 199 of (collapsed) Gibbs sampling\n",
      "Iteration 200 of (collapsed) Gibbs sampling\n",
      "Iteration 201 of (collapsed) Gibbs sampling\n",
      "Iteration 202 of (collapsed) Gibbs sampling\n",
      "Iteration 203 of (collapsed) Gibbs sampling\n",
      "Iteration 204 of (collapsed) Gibbs sampling\n",
      "Iteration 205 of (collapsed) Gibbs sampling\n",
      "Iteration 206 of (collapsed) Gibbs sampling\n",
      "Iteration 207 of (collapsed) Gibbs sampling\n",
      "Iteration 208 of (collapsed) Gibbs sampling\n",
      "Iteration 209 of (collapsed) Gibbs sampling\n",
      "Iteration 210 of (collapsed) Gibbs sampling\n",
      "Iteration 211 of (collapsed) Gibbs sampling\n",
      "Iteration 212 of (collapsed) Gibbs sampling\n",
      "Iteration 213 of (collapsed) Gibbs sampling\n",
      "Iteration 214 of (collapsed) Gibbs sampling\n",
      "Iteration 215 of (collapsed) Gibbs sampling\n",
      "Iteration 216 of (collapsed) Gibbs sampling\n",
      "Iteration 217 of (collapsed) Gibbs sampling\n",
      "Iteration 218 of (collapsed) Gibbs sampling\n",
      "Iteration 219 of (collapsed) Gibbs sampling\n",
      "Iteration 220 of (collapsed) Gibbs sampling\n",
      "Iteration 221 of (collapsed) Gibbs sampling\n",
      "Iteration 222 of (collapsed) Gibbs sampling\n",
      "Iteration 223 of (collapsed) Gibbs sampling\n",
      "Iteration 224 of (collapsed) Gibbs sampling\n",
      "Iteration 225 of (collapsed) Gibbs sampling\n",
      "Iteration 226 of (collapsed) Gibbs sampling\n",
      "Iteration 227 of (collapsed) Gibbs sampling\n",
      "Iteration 228 of (collapsed) Gibbs sampling\n",
      "Iteration 229 of (collapsed) Gibbs sampling\n",
      "Iteration 230 of (collapsed) Gibbs sampling\n",
      "Iteration 231 of (collapsed) Gibbs sampling\n",
      "Iteration 232 of (collapsed) Gibbs sampling\n",
      "Iteration 233 of (collapsed) Gibbs sampling\n",
      "Iteration 234 of (collapsed) Gibbs sampling\n",
      "Iteration 235 of (collapsed) Gibbs sampling\n",
      "Iteration 236 of (collapsed) Gibbs sampling\n",
      "Iteration 237 of (collapsed) Gibbs sampling\n",
      "Iteration 238 of (collapsed) Gibbs sampling\n",
      "Iteration 239 of (collapsed) Gibbs sampling\n",
      "Iteration 240 of (collapsed) Gibbs sampling\n",
      "Iteration 241 of (collapsed) Gibbs sampling\n",
      "Iteration 242 of (collapsed) Gibbs sampling\n",
      "Iteration 243 of (collapsed) Gibbs sampling\n",
      "Iteration 244 of (collapsed) Gibbs sampling\n",
      "Iteration 245 of (collapsed) Gibbs sampling\n",
      "Iteration 246 of (collapsed) Gibbs sampling\n",
      "Iteration 247 of (collapsed) Gibbs sampling\n",
      "Iteration 248 of (collapsed) Gibbs sampling\n",
      "Iteration 249 of (collapsed) Gibbs sampling\n",
      "Iteration 250 of (collapsed) Gibbs sampling\n",
      "Iteration 251 of (collapsed) Gibbs sampling\n",
      "Iteration 252 of (collapsed) Gibbs sampling\n",
      "Iteration 253 of (collapsed) Gibbs sampling\n",
      "Iteration 254 of (collapsed) Gibbs sampling\n",
      "Iteration 255 of (collapsed) Gibbs sampling\n",
      "Iteration 256 of (collapsed) Gibbs sampling\n",
      "Iteration 257 of (collapsed) Gibbs sampling\n",
      "Iteration 258 of (collapsed) Gibbs sampling\n",
      "Iteration 259 of (collapsed) Gibbs sampling\n",
      "Iteration 260 of (collapsed) Gibbs sampling\n",
      "Iteration 261 of (collapsed) Gibbs sampling\n",
      "Iteration 262 of (collapsed) Gibbs sampling\n",
      "Iteration 263 of (collapsed) Gibbs sampling\n",
      "Iteration 264 of (collapsed) Gibbs sampling\n",
      "Iteration 265 of (collapsed) Gibbs sampling\n",
      "Iteration 266 of (collapsed) Gibbs sampling\n",
      "Iteration 267 of (collapsed) Gibbs sampling\n",
      "Iteration 268 of (collapsed) Gibbs sampling\n",
      "Iteration 269 of (collapsed) Gibbs sampling\n",
      "Iteration 270 of (collapsed) Gibbs sampling\n",
      "Iteration 271 of (collapsed) Gibbs sampling\n",
      "Iteration 272 of (collapsed) Gibbs sampling\n",
      "Iteration 273 of (collapsed) Gibbs sampling\n",
      "Iteration 274 of (collapsed) Gibbs sampling\n",
      "Iteration 275 of (collapsed) Gibbs sampling\n",
      "Iteration 276 of (collapsed) Gibbs sampling\n",
      "Iteration 277 of (collapsed) Gibbs sampling\n",
      "Iteration 278 of (collapsed) Gibbs sampling\n",
      "Iteration 279 of (collapsed) Gibbs sampling\n",
      "Iteration 280 of (collapsed) Gibbs sampling\n",
      "Iteration 281 of (collapsed) Gibbs sampling\n",
      "Iteration 282 of (collapsed) Gibbs sampling\n",
      "Iteration 283 of (collapsed) Gibbs sampling\n",
      "Iteration 284 of (collapsed) Gibbs sampling\n",
      "Iteration 285 of (collapsed) Gibbs sampling\n",
      "Iteration 286 of (collapsed) Gibbs sampling\n",
      "Iteration 287 of (collapsed) Gibbs sampling\n",
      "Iteration 288 of (collapsed) Gibbs sampling\n",
      "Iteration 289 of (collapsed) Gibbs sampling\n",
      "Iteration 290 of (collapsed) Gibbs sampling\n",
      "Iteration 291 of (collapsed) Gibbs sampling\n",
      "Iteration 292 of (collapsed) Gibbs sampling\n",
      "Iteration 293 of (collapsed) Gibbs sampling\n",
      "Iteration 294 of (collapsed) Gibbs sampling\n",
      "Iteration 295 of (collapsed) Gibbs sampling\n",
      "Iteration 296 of (collapsed) Gibbs sampling\n",
      "Iteration 297 of (collapsed) Gibbs sampling\n",
      "Iteration 298 of (collapsed) Gibbs sampling\n",
      "Iteration 299 of (collapsed) Gibbs sampling\n",
      "Iteration 300 of (collapsed) Gibbs sampling\n",
      "Iteration 301 of (collapsed) Gibbs sampling\n",
      "Iteration 302 of (collapsed) Gibbs sampling\n",
      "Iteration 303 of (collapsed) Gibbs sampling\n",
      "Iteration 304 of (collapsed) Gibbs sampling\n",
      "Iteration 305 of (collapsed) Gibbs sampling\n",
      "Iteration 306 of (collapsed) Gibbs sampling\n",
      "Iteration 307 of (collapsed) Gibbs sampling\n",
      "Iteration 308 of (collapsed) Gibbs sampling\n",
      "Iteration 309 of (collapsed) Gibbs sampling\n",
      "Iteration 310 of (collapsed) Gibbs sampling\n",
      "Iteration 311 of (collapsed) Gibbs sampling\n",
      "Iteration 312 of (collapsed) Gibbs sampling\n",
      "Iteration 313 of (collapsed) Gibbs sampling\n",
      "Iteration 314 of (collapsed) Gibbs sampling\n",
      "Iteration 315 of (collapsed) Gibbs sampling\n",
      "Iteration 316 of (collapsed) Gibbs sampling\n",
      "Iteration 317 of (collapsed) Gibbs sampling\n",
      "Iteration 318 of (collapsed) Gibbs sampling\n",
      "Iteration 319 of (collapsed) Gibbs sampling\n",
      "Iteration 320 of (collapsed) Gibbs sampling\n",
      "Iteration 321 of (collapsed) Gibbs sampling\n",
      "Iteration 322 of (collapsed) Gibbs sampling\n",
      "Iteration 323 of (collapsed) Gibbs sampling\n",
      "Iteration 324 of (collapsed) Gibbs sampling\n",
      "Iteration 325 of (collapsed) Gibbs sampling\n",
      "Iteration 326 of (collapsed) Gibbs sampling\n",
      "Iteration 327 of (collapsed) Gibbs sampling\n",
      "Iteration 328 of (collapsed) Gibbs sampling\n",
      "Iteration 329 of (collapsed) Gibbs sampling\n",
      "Iteration 330 of (collapsed) Gibbs sampling\n",
      "Iteration 331 of (collapsed) Gibbs sampling\n",
      "Iteration 332 of (collapsed) Gibbs sampling\n",
      "Iteration 333 of (collapsed) Gibbs sampling\n",
      "Iteration 334 of (collapsed) Gibbs sampling\n",
      "Iteration 335 of (collapsed) Gibbs sampling\n",
      "Iteration 336 of (collapsed) Gibbs sampling\n",
      "Iteration 337 of (collapsed) Gibbs sampling\n",
      "Iteration 338 of (collapsed) Gibbs sampling\n",
      "Iteration 339 of (collapsed) Gibbs sampling\n",
      "Iteration 340 of (collapsed) Gibbs sampling\n",
      "Iteration 341 of (collapsed) Gibbs sampling\n",
      "Iteration 342 of (collapsed) Gibbs sampling\n",
      "Iteration 343 of (collapsed) Gibbs sampling\n",
      "Iteration 344 of (collapsed) Gibbs sampling\n",
      "Iteration 345 of (collapsed) Gibbs sampling\n",
      "Iteration 346 of (collapsed) Gibbs sampling\n",
      "Iteration 347 of (collapsed) Gibbs sampling\n",
      "Iteration 348 of (collapsed) Gibbs sampling\n",
      "Iteration 349 of (collapsed) Gibbs sampling\n",
      "Iteration 350 of (collapsed) Gibbs sampling\n",
      "Iteration 351 of (collapsed) Gibbs sampling\n",
      "Iteration 352 of (collapsed) Gibbs sampling\n",
      "Iteration 353 of (collapsed) Gibbs sampling\n",
      "Iteration 354 of (collapsed) Gibbs sampling\n",
      "Iteration 355 of (collapsed) Gibbs sampling\n",
      "Iteration 356 of (collapsed) Gibbs sampling\n",
      "Iteration 357 of (collapsed) Gibbs sampling\n",
      "Iteration 358 of (collapsed) Gibbs sampling\n",
      "Iteration 359 of (collapsed) Gibbs sampling\n",
      "Iteration 360 of (collapsed) Gibbs sampling\n",
      "Iteration 361 of (collapsed) Gibbs sampling\n",
      "Iteration 362 of (collapsed) Gibbs sampling\n",
      "Iteration 363 of (collapsed) Gibbs sampling\n",
      "Iteration 364 of (collapsed) Gibbs sampling\n",
      "Iteration 365 of (collapsed) Gibbs sampling\n",
      "Iteration 366 of (collapsed) Gibbs sampling\n",
      "Iteration 367 of (collapsed) Gibbs sampling\n",
      "Iteration 368 of (collapsed) Gibbs sampling\n",
      "Iteration 369 of (collapsed) Gibbs sampling\n",
      "Iteration 370 of (collapsed) Gibbs sampling\n",
      "Iteration 371 of (collapsed) Gibbs sampling\n",
      "Iteration 372 of (collapsed) Gibbs sampling\n",
      "Iteration 373 of (collapsed) Gibbs sampling\n",
      "Iteration 374 of (collapsed) Gibbs sampling\n",
      "Iteration 375 of (collapsed) Gibbs sampling\n",
      "Iteration 376 of (collapsed) Gibbs sampling\n",
      "Iteration 377 of (collapsed) Gibbs sampling\n",
      "Iteration 378 of (collapsed) Gibbs sampling\n",
      "Iteration 379 of (collapsed) Gibbs sampling\n",
      "Iteration 380 of (collapsed) Gibbs sampling\n",
      "Iteration 381 of (collapsed) Gibbs sampling\n",
      "Iteration 382 of (collapsed) Gibbs sampling\n",
      "Iteration 383 of (collapsed) Gibbs sampling\n",
      "Iteration 384 of (collapsed) Gibbs sampling\n",
      "Iteration 385 of (collapsed) Gibbs sampling\n",
      "Iteration 386 of (collapsed) Gibbs sampling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 387 of (collapsed) Gibbs sampling\n",
      "Iteration 388 of (collapsed) Gibbs sampling\n",
      "Iteration 389 of (collapsed) Gibbs sampling\n",
      "Iteration 390 of (collapsed) Gibbs sampling\n",
      "Iteration 391 of (collapsed) Gibbs sampling\n",
      "Iteration 392 of (collapsed) Gibbs sampling\n",
      "Iteration 393 of (collapsed) Gibbs sampling\n",
      "Iteration 394 of (collapsed) Gibbs sampling\n",
      "Iteration 395 of (collapsed) Gibbs sampling\n",
      "Iteration 396 of (collapsed) Gibbs sampling\n",
      "Iteration 397 of (collapsed) Gibbs sampling\n",
      "Iteration 398 of (collapsed) Gibbs sampling\n",
      "Iteration 399 of (collapsed) Gibbs sampling\n",
      "Iteration 400 of (collapsed) Gibbs sampling\n",
      "Iteration 401 of (collapsed) Gibbs sampling\n",
      "Iteration 402 of (collapsed) Gibbs sampling\n",
      "Iteration 403 of (collapsed) Gibbs sampling\n",
      "Iteration 404 of (collapsed) Gibbs sampling\n",
      "Iteration 405 of (collapsed) Gibbs sampling\n",
      "Iteration 406 of (collapsed) Gibbs sampling\n",
      "Iteration 407 of (collapsed) Gibbs sampling\n",
      "Iteration 408 of (collapsed) Gibbs sampling\n",
      "Iteration 409 of (collapsed) Gibbs sampling\n",
      "Iteration 410 of (collapsed) Gibbs sampling\n",
      "Iteration 411 of (collapsed) Gibbs sampling\n",
      "Iteration 412 of (collapsed) Gibbs sampling\n",
      "Iteration 413 of (collapsed) Gibbs sampling\n",
      "Iteration 414 of (collapsed) Gibbs sampling\n",
      "Iteration 415 of (collapsed) Gibbs sampling\n",
      "Iteration 416 of (collapsed) Gibbs sampling\n",
      "Iteration 417 of (collapsed) Gibbs sampling\n",
      "Iteration 418 of (collapsed) Gibbs sampling\n",
      "Iteration 419 of (collapsed) Gibbs sampling\n",
      "Iteration 420 of (collapsed) Gibbs sampling\n",
      "Iteration 421 of (collapsed) Gibbs sampling\n",
      "Iteration 422 of (collapsed) Gibbs sampling\n",
      "Iteration 423 of (collapsed) Gibbs sampling\n",
      "Iteration 424 of (collapsed) Gibbs sampling\n",
      "Iteration 425 of (collapsed) Gibbs sampling\n",
      "Iteration 426 of (collapsed) Gibbs sampling\n",
      "Iteration 427 of (collapsed) Gibbs sampling\n",
      "Iteration 428 of (collapsed) Gibbs sampling\n",
      "Iteration 429 of (collapsed) Gibbs sampling\n",
      "Iteration 430 of (collapsed) Gibbs sampling\n",
      "Iteration 431 of (collapsed) Gibbs sampling\n",
      "Iteration 432 of (collapsed) Gibbs sampling\n",
      "Iteration 433 of (collapsed) Gibbs sampling\n",
      "Iteration 434 of (collapsed) Gibbs sampling\n",
      "Iteration 435 of (collapsed) Gibbs sampling\n",
      "Iteration 436 of (collapsed) Gibbs sampling\n",
      "Iteration 437 of (collapsed) Gibbs sampling\n",
      "Iteration 438 of (collapsed) Gibbs sampling\n",
      "Iteration 439 of (collapsed) Gibbs sampling\n",
      "Iteration 440 of (collapsed) Gibbs sampling\n",
      "Iteration 441 of (collapsed) Gibbs sampling\n",
      "Iteration 442 of (collapsed) Gibbs sampling\n",
      "Iteration 443 of (collapsed) Gibbs sampling\n",
      "Iteration 444 of (collapsed) Gibbs sampling\n",
      "Iteration 445 of (collapsed) Gibbs sampling\n",
      "Iteration 446 of (collapsed) Gibbs sampling\n",
      "Iteration 447 of (collapsed) Gibbs sampling\n",
      "Iteration 448 of (collapsed) Gibbs sampling\n",
      "Iteration 449 of (collapsed) Gibbs sampling\n",
      "Iteration 450 of (collapsed) Gibbs sampling\n",
      "Iteration 451 of (collapsed) Gibbs sampling\n",
      "Iteration 452 of (collapsed) Gibbs sampling\n",
      "Iteration 453 of (collapsed) Gibbs sampling\n",
      "Iteration 454 of (collapsed) Gibbs sampling\n",
      "Iteration 455 of (collapsed) Gibbs sampling\n",
      "Iteration 456 of (collapsed) Gibbs sampling\n",
      "Iteration 457 of (collapsed) Gibbs sampling\n",
      "Iteration 458 of (collapsed) Gibbs sampling\n",
      "Iteration 459 of (collapsed) Gibbs sampling\n",
      "Iteration 460 of (collapsed) Gibbs sampling\n",
      "Iteration 461 of (collapsed) Gibbs sampling\n",
      "Iteration 462 of (collapsed) Gibbs sampling\n",
      "Iteration 463 of (collapsed) Gibbs sampling\n",
      "Iteration 464 of (collapsed) Gibbs sampling\n",
      "Iteration 465 of (collapsed) Gibbs sampling\n",
      "Iteration 466 of (collapsed) Gibbs sampling\n",
      "Iteration 467 of (collapsed) Gibbs sampling\n",
      "Iteration 468 of (collapsed) Gibbs sampling\n",
      "Iteration 469 of (collapsed) Gibbs sampling\n",
      "Iteration 470 of (collapsed) Gibbs sampling\n",
      "Iteration 471 of (collapsed) Gibbs sampling\n",
      "Iteration 472 of (collapsed) Gibbs sampling\n",
      "Iteration 473 of (collapsed) Gibbs sampling\n",
      "Iteration 474 of (collapsed) Gibbs sampling\n",
      "Iteration 475 of (collapsed) Gibbs sampling\n",
      "Iteration 476 of (collapsed) Gibbs sampling\n",
      "Iteration 477 of (collapsed) Gibbs sampling\n",
      "Iteration 478 of (collapsed) Gibbs sampling\n",
      "Iteration 479 of (collapsed) Gibbs sampling\n",
      "Iteration 480 of (collapsed) Gibbs sampling\n",
      "Iteration 481 of (collapsed) Gibbs sampling\n",
      "Iteration 482 of (collapsed) Gibbs sampling\n",
      "Iteration 483 of (collapsed) Gibbs sampling\n",
      "Iteration 484 of (collapsed) Gibbs sampling\n",
      "Iteration 485 of (collapsed) Gibbs sampling\n",
      "Iteration 486 of (collapsed) Gibbs sampling\n",
      "Iteration 487 of (collapsed) Gibbs sampling\n",
      "Iteration 488 of (collapsed) Gibbs sampling\n",
      "Iteration 489 of (collapsed) Gibbs sampling\n",
      "Iteration 490 of (collapsed) Gibbs sampling\n",
      "Iteration 491 of (collapsed) Gibbs sampling\n",
      "Iteration 492 of (collapsed) Gibbs sampling\n",
      "Iteration 493 of (collapsed) Gibbs sampling\n",
      "Iteration 494 of (collapsed) Gibbs sampling\n",
      "Iteration 495 of (collapsed) Gibbs sampling\n",
      "Iteration 496 of (collapsed) Gibbs sampling\n",
      "Iteration 497 of (collapsed) Gibbs sampling\n",
      "Iteration 498 of (collapsed) Gibbs sampling\n",
      "Iteration 499 of (collapsed) Gibbs sampling\n",
      "Iteration 500 of (collapsed) Gibbs sampling\n"
     ]
    }
   ],
   "source": [
    "ldaobj.sample(0,50,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we allowed no burn in and started sampling straight away, one would imagine the initial draws were poor in terms of describing topics.  A formalization of this idea is to compute the perplexity of each of the samples.  Perplexity is a common goodness-of-fit meausure in natural language processing and information theory literature that describes how well a probability model explains data.  Lower values indicate better goodness-of-fit.  Calling ldaobj.perplexity() returns the perplexity of each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 923.89691508,  880.54024219,  866.29460983,  855.28670368,\n",
       "        851.03009011,  850.92834905,  845.96828092,  845.57143712,\n",
       "        844.06261119,  844.70933996])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldaobj.perplexity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as we suspected, the first sample has a much higher perplexity than the last.  Moreover, it might be that if we had kept sampling the chain, we could get even lower perplexity.  Once we call ldaobj.sample the first time, all further calls extend the existing chain by default rather than start from scratch.  So let's draw another ten samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 of (collapsed) Gibbs sampling\n",
      "Iteration 2 of (collapsed) Gibbs sampling\n",
      "Iteration 3 of (collapsed) Gibbs sampling\n",
      "Iteration 4 of (collapsed) Gibbs sampling\n",
      "Iteration 5 of (collapsed) Gibbs sampling\n",
      "Iteration 6 of (collapsed) Gibbs sampling\n",
      "Iteration 7 of (collapsed) Gibbs sampling\n",
      "Iteration 8 of (collapsed) Gibbs sampling\n",
      "Iteration 9 of (collapsed) Gibbs sampling\n",
      "Iteration 10 of (collapsed) Gibbs sampling\n",
      "Iteration 11 of (collapsed) Gibbs sampling\n",
      "Iteration 12 of (collapsed) Gibbs sampling\n",
      "Iteration 13 of (collapsed) Gibbs sampling\n",
      "Iteration 14 of (collapsed) Gibbs sampling\n",
      "Iteration 15 of (collapsed) Gibbs sampling\n",
      "Iteration 16 of (collapsed) Gibbs sampling\n",
      "Iteration 17 of (collapsed) Gibbs sampling\n",
      "Iteration 18 of (collapsed) Gibbs sampling\n",
      "Iteration 19 of (collapsed) Gibbs sampling\n",
      "Iteration 20 of (collapsed) Gibbs sampling\n",
      "Iteration 21 of (collapsed) Gibbs sampling\n",
      "Iteration 22 of (collapsed) Gibbs sampling\n",
      "Iteration 23 of (collapsed) Gibbs sampling\n",
      "Iteration 24 of (collapsed) Gibbs sampling\n",
      "Iteration 25 of (collapsed) Gibbs sampling\n",
      "Iteration 26 of (collapsed) Gibbs sampling\n",
      "Iteration 27 of (collapsed) Gibbs sampling\n",
      "Iteration 28 of (collapsed) Gibbs sampling\n",
      "Iteration 29 of (collapsed) Gibbs sampling\n",
      "Iteration 30 of (collapsed) Gibbs sampling\n",
      "Iteration 31 of (collapsed) Gibbs sampling\n",
      "Iteration 32 of (collapsed) Gibbs sampling\n",
      "Iteration 33 of (collapsed) Gibbs sampling\n",
      "Iteration 34 of (collapsed) Gibbs sampling\n",
      "Iteration 35 of (collapsed) Gibbs sampling\n",
      "Iteration 36 of (collapsed) Gibbs sampling\n",
      "Iteration 37 of (collapsed) Gibbs sampling\n",
      "Iteration 38 of (collapsed) Gibbs sampling\n",
      "Iteration 39 of (collapsed) Gibbs sampling\n",
      "Iteration 40 of (collapsed) Gibbs sampling\n",
      "Iteration 41 of (collapsed) Gibbs sampling\n",
      "Iteration 42 of (collapsed) Gibbs sampling\n",
      "Iteration 43 of (collapsed) Gibbs sampling\n",
      "Iteration 44 of (collapsed) Gibbs sampling\n",
      "Iteration 45 of (collapsed) Gibbs sampling\n",
      "Iteration 46 of (collapsed) Gibbs sampling\n",
      "Iteration 47 of (collapsed) Gibbs sampling\n",
      "Iteration 48 of (collapsed) Gibbs sampling\n",
      "Iteration 49 of (collapsed) Gibbs sampling\n",
      "Iteration 50 of (collapsed) Gibbs sampling\n",
      "Iteration 51 of (collapsed) Gibbs sampling\n",
      "Iteration 52 of (collapsed) Gibbs sampling\n",
      "Iteration 53 of (collapsed) Gibbs sampling\n",
      "Iteration 54 of (collapsed) Gibbs sampling\n",
      "Iteration 55 of (collapsed) Gibbs sampling\n",
      "Iteration 56 of (collapsed) Gibbs sampling\n",
      "Iteration 57 of (collapsed) Gibbs sampling\n",
      "Iteration 58 of (collapsed) Gibbs sampling\n",
      "Iteration 59 of (collapsed) Gibbs sampling\n",
      "Iteration 60 of (collapsed) Gibbs sampling\n",
      "Iteration 61 of (collapsed) Gibbs sampling\n",
      "Iteration 62 of (collapsed) Gibbs sampling\n",
      "Iteration 63 of (collapsed) Gibbs sampling\n",
      "Iteration 64 of (collapsed) Gibbs sampling\n",
      "Iteration 65 of (collapsed) Gibbs sampling\n",
      "Iteration 66 of (collapsed) Gibbs sampling\n",
      "Iteration 67 of (collapsed) Gibbs sampling\n",
      "Iteration 68 of (collapsed) Gibbs sampling\n",
      "Iteration 69 of (collapsed) Gibbs sampling\n",
      "Iteration 70 of (collapsed) Gibbs sampling\n",
      "Iteration 71 of (collapsed) Gibbs sampling\n",
      "Iteration 72 of (collapsed) Gibbs sampling\n",
      "Iteration 73 of (collapsed) Gibbs sampling\n",
      "Iteration 74 of (collapsed) Gibbs sampling\n",
      "Iteration 75 of (collapsed) Gibbs sampling\n",
      "Iteration 76 of (collapsed) Gibbs sampling\n",
      "Iteration 77 of (collapsed) Gibbs sampling\n",
      "Iteration 78 of (collapsed) Gibbs sampling\n",
      "Iteration 79 of (collapsed) Gibbs sampling\n",
      "Iteration 80 of (collapsed) Gibbs sampling\n",
      "Iteration 81 of (collapsed) Gibbs sampling\n",
      "Iteration 82 of (collapsed) Gibbs sampling\n",
      "Iteration 83 of (collapsed) Gibbs sampling\n",
      "Iteration 84 of (collapsed) Gibbs sampling\n",
      "Iteration 85 of (collapsed) Gibbs sampling\n",
      "Iteration 86 of (collapsed) Gibbs sampling\n",
      "Iteration 87 of (collapsed) Gibbs sampling\n",
      "Iteration 88 of (collapsed) Gibbs sampling\n",
      "Iteration 89 of (collapsed) Gibbs sampling\n",
      "Iteration 90 of (collapsed) Gibbs sampling\n",
      "Iteration 91 of (collapsed) Gibbs sampling\n",
      "Iteration 92 of (collapsed) Gibbs sampling\n",
      "Iteration 93 of (collapsed) Gibbs sampling\n",
      "Iteration 94 of (collapsed) Gibbs sampling\n",
      "Iteration 95 of (collapsed) Gibbs sampling\n",
      "Iteration 96 of (collapsed) Gibbs sampling\n",
      "Iteration 97 of (collapsed) Gibbs sampling\n",
      "Iteration 98 of (collapsed) Gibbs sampling\n",
      "Iteration 99 of (collapsed) Gibbs sampling\n",
      "Iteration 100 of (collapsed) Gibbs sampling\n",
      "Iteration 101 of (collapsed) Gibbs sampling\n",
      "Iteration 102 of (collapsed) Gibbs sampling\n",
      "Iteration 103 of (collapsed) Gibbs sampling\n",
      "Iteration 104 of (collapsed) Gibbs sampling\n",
      "Iteration 105 of (collapsed) Gibbs sampling\n",
      "Iteration 106 of (collapsed) Gibbs sampling\n",
      "Iteration 107 of (collapsed) Gibbs sampling\n",
      "Iteration 108 of (collapsed) Gibbs sampling\n",
      "Iteration 109 of (collapsed) Gibbs sampling\n",
      "Iteration 110 of (collapsed) Gibbs sampling\n",
      "Iteration 111 of (collapsed) Gibbs sampling\n",
      "Iteration 112 of (collapsed) Gibbs sampling\n",
      "Iteration 113 of (collapsed) Gibbs sampling\n",
      "Iteration 114 of (collapsed) Gibbs sampling\n",
      "Iteration 115 of (collapsed) Gibbs sampling\n",
      "Iteration 116 of (collapsed) Gibbs sampling\n",
      "Iteration 117 of (collapsed) Gibbs sampling\n",
      "Iteration 118 of (collapsed) Gibbs sampling\n",
      "Iteration 119 of (collapsed) Gibbs sampling\n",
      "Iteration 120 of (collapsed) Gibbs sampling\n",
      "Iteration 121 of (collapsed) Gibbs sampling\n",
      "Iteration 122 of (collapsed) Gibbs sampling\n",
      "Iteration 123 of (collapsed) Gibbs sampling\n",
      "Iteration 124 of (collapsed) Gibbs sampling\n",
      "Iteration 125 of (collapsed) Gibbs sampling\n",
      "Iteration 126 of (collapsed) Gibbs sampling\n",
      "Iteration 127 of (collapsed) Gibbs sampling\n",
      "Iteration 128 of (collapsed) Gibbs sampling\n",
      "Iteration 129 of (collapsed) Gibbs sampling\n",
      "Iteration 130 of (collapsed) Gibbs sampling\n",
      "Iteration 131 of (collapsed) Gibbs sampling\n",
      "Iteration 132 of (collapsed) Gibbs sampling\n",
      "Iteration 133 of (collapsed) Gibbs sampling\n",
      "Iteration 134 of (collapsed) Gibbs sampling\n",
      "Iteration 135 of (collapsed) Gibbs sampling\n",
      "Iteration 136 of (collapsed) Gibbs sampling\n",
      "Iteration 137 of (collapsed) Gibbs sampling\n",
      "Iteration 138 of (collapsed) Gibbs sampling\n",
      "Iteration 139 of (collapsed) Gibbs sampling\n",
      "Iteration 140 of (collapsed) Gibbs sampling\n",
      "Iteration 141 of (collapsed) Gibbs sampling\n",
      "Iteration 142 of (collapsed) Gibbs sampling\n",
      "Iteration 143 of (collapsed) Gibbs sampling\n",
      "Iteration 144 of (collapsed) Gibbs sampling\n",
      "Iteration 145 of (collapsed) Gibbs sampling\n",
      "Iteration 146 of (collapsed) Gibbs sampling\n",
      "Iteration 147 of (collapsed) Gibbs sampling\n",
      "Iteration 148 of (collapsed) Gibbs sampling\n",
      "Iteration 149 of (collapsed) Gibbs sampling\n",
      "Iteration 150 of (collapsed) Gibbs sampling\n",
      "Iteration 151 of (collapsed) Gibbs sampling\n",
      "Iteration 152 of (collapsed) Gibbs sampling\n",
      "Iteration 153 of (collapsed) Gibbs sampling\n",
      "Iteration 154 of (collapsed) Gibbs sampling\n",
      "Iteration 155 of (collapsed) Gibbs sampling\n",
      "Iteration 156 of (collapsed) Gibbs sampling\n",
      "Iteration 157 of (collapsed) Gibbs sampling\n",
      "Iteration 158 of (collapsed) Gibbs sampling\n",
      "Iteration 159 of (collapsed) Gibbs sampling\n",
      "Iteration 160 of (collapsed) Gibbs sampling\n",
      "Iteration 161 of (collapsed) Gibbs sampling\n",
      "Iteration 162 of (collapsed) Gibbs sampling\n",
      "Iteration 163 of (collapsed) Gibbs sampling\n",
      "Iteration 164 of (collapsed) Gibbs sampling\n",
      "Iteration 165 of (collapsed) Gibbs sampling\n",
      "Iteration 166 of (collapsed) Gibbs sampling\n",
      "Iteration 167 of (collapsed) Gibbs sampling\n",
      "Iteration 168 of (collapsed) Gibbs sampling\n",
      "Iteration 169 of (collapsed) Gibbs sampling\n",
      "Iteration 170 of (collapsed) Gibbs sampling\n",
      "Iteration 171 of (collapsed) Gibbs sampling\n",
      "Iteration 172 of (collapsed) Gibbs sampling\n",
      "Iteration 173 of (collapsed) Gibbs sampling\n",
      "Iteration 174 of (collapsed) Gibbs sampling\n",
      "Iteration 175 of (collapsed) Gibbs sampling\n",
      "Iteration 176 of (collapsed) Gibbs sampling\n",
      "Iteration 177 of (collapsed) Gibbs sampling\n",
      "Iteration 178 of (collapsed) Gibbs sampling\n",
      "Iteration 179 of (collapsed) Gibbs sampling\n",
      "Iteration 180 of (collapsed) Gibbs sampling\n",
      "Iteration 181 of (collapsed) Gibbs sampling\n",
      "Iteration 182 of (collapsed) Gibbs sampling\n",
      "Iteration 183 of (collapsed) Gibbs sampling\n",
      "Iteration 184 of (collapsed) Gibbs sampling\n",
      "Iteration 185 of (collapsed) Gibbs sampling\n",
      "Iteration 186 of (collapsed) Gibbs sampling\n",
      "Iteration 187 of (collapsed) Gibbs sampling\n",
      "Iteration 188 of (collapsed) Gibbs sampling\n",
      "Iteration 189 of (collapsed) Gibbs sampling\n",
      "Iteration 190 of (collapsed) Gibbs sampling\n",
      "Iteration 191 of (collapsed) Gibbs sampling\n",
      "Iteration 192 of (collapsed) Gibbs sampling\n",
      "Iteration 193 of (collapsed) Gibbs sampling\n",
      "Iteration 194 of (collapsed) Gibbs sampling\n",
      "Iteration 195 of (collapsed) Gibbs sampling\n",
      "Iteration 196 of (collapsed) Gibbs sampling\n",
      "Iteration 197 of (collapsed) Gibbs sampling\n",
      "Iteration 198 of (collapsed) Gibbs sampling\n",
      "Iteration 199 of (collapsed) Gibbs sampling\n",
      "Iteration 200 of (collapsed) Gibbs sampling\n",
      "Iteration 201 of (collapsed) Gibbs sampling\n",
      "Iteration 202 of (collapsed) Gibbs sampling\n",
      "Iteration 203 of (collapsed) Gibbs sampling\n",
      "Iteration 204 of (collapsed) Gibbs sampling\n",
      "Iteration 205 of (collapsed) Gibbs sampling\n",
      "Iteration 206 of (collapsed) Gibbs sampling\n",
      "Iteration 207 of (collapsed) Gibbs sampling\n",
      "Iteration 208 of (collapsed) Gibbs sampling\n",
      "Iteration 209 of (collapsed) Gibbs sampling\n",
      "Iteration 210 of (collapsed) Gibbs sampling\n",
      "Iteration 211 of (collapsed) Gibbs sampling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 212 of (collapsed) Gibbs sampling\n",
      "Iteration 213 of (collapsed) Gibbs sampling\n",
      "Iteration 214 of (collapsed) Gibbs sampling\n",
      "Iteration 215 of (collapsed) Gibbs sampling\n",
      "Iteration 216 of (collapsed) Gibbs sampling\n",
      "Iteration 217 of (collapsed) Gibbs sampling\n",
      "Iteration 218 of (collapsed) Gibbs sampling\n",
      "Iteration 219 of (collapsed) Gibbs sampling\n",
      "Iteration 220 of (collapsed) Gibbs sampling\n",
      "Iteration 221 of (collapsed) Gibbs sampling\n",
      "Iteration 222 of (collapsed) Gibbs sampling\n",
      "Iteration 223 of (collapsed) Gibbs sampling\n",
      "Iteration 224 of (collapsed) Gibbs sampling\n",
      "Iteration 225 of (collapsed) Gibbs sampling\n",
      "Iteration 226 of (collapsed) Gibbs sampling\n",
      "Iteration 227 of (collapsed) Gibbs sampling\n",
      "Iteration 228 of (collapsed) Gibbs sampling\n",
      "Iteration 229 of (collapsed) Gibbs sampling\n",
      "Iteration 230 of (collapsed) Gibbs sampling\n",
      "Iteration 231 of (collapsed) Gibbs sampling\n",
      "Iteration 232 of (collapsed) Gibbs sampling\n",
      "Iteration 233 of (collapsed) Gibbs sampling\n",
      "Iteration 234 of (collapsed) Gibbs sampling\n",
      "Iteration 235 of (collapsed) Gibbs sampling\n",
      "Iteration 236 of (collapsed) Gibbs sampling\n",
      "Iteration 237 of (collapsed) Gibbs sampling\n",
      "Iteration 238 of (collapsed) Gibbs sampling\n",
      "Iteration 239 of (collapsed) Gibbs sampling\n",
      "Iteration 240 of (collapsed) Gibbs sampling\n",
      "Iteration 241 of (collapsed) Gibbs sampling\n",
      "Iteration 242 of (collapsed) Gibbs sampling\n",
      "Iteration 243 of (collapsed) Gibbs sampling\n",
      "Iteration 244 of (collapsed) Gibbs sampling\n",
      "Iteration 245 of (collapsed) Gibbs sampling\n",
      "Iteration 246 of (collapsed) Gibbs sampling\n",
      "Iteration 247 of (collapsed) Gibbs sampling\n",
      "Iteration 248 of (collapsed) Gibbs sampling\n",
      "Iteration 249 of (collapsed) Gibbs sampling\n",
      "Iteration 250 of (collapsed) Gibbs sampling\n",
      "Iteration 251 of (collapsed) Gibbs sampling\n",
      "Iteration 252 of (collapsed) Gibbs sampling\n",
      "Iteration 253 of (collapsed) Gibbs sampling\n",
      "Iteration 254 of (collapsed) Gibbs sampling\n",
      "Iteration 255 of (collapsed) Gibbs sampling\n",
      "Iteration 256 of (collapsed) Gibbs sampling\n",
      "Iteration 257 of (collapsed) Gibbs sampling\n",
      "Iteration 258 of (collapsed) Gibbs sampling\n",
      "Iteration 259 of (collapsed) Gibbs sampling\n",
      "Iteration 260 of (collapsed) Gibbs sampling\n",
      "Iteration 261 of (collapsed) Gibbs sampling\n",
      "Iteration 262 of (collapsed) Gibbs sampling\n",
      "Iteration 263 of (collapsed) Gibbs sampling\n",
      "Iteration 264 of (collapsed) Gibbs sampling\n",
      "Iteration 265 of (collapsed) Gibbs sampling\n",
      "Iteration 266 of (collapsed) Gibbs sampling\n",
      "Iteration 267 of (collapsed) Gibbs sampling\n",
      "Iteration 268 of (collapsed) Gibbs sampling\n",
      "Iteration 269 of (collapsed) Gibbs sampling\n",
      "Iteration 270 of (collapsed) Gibbs sampling\n",
      "Iteration 271 of (collapsed) Gibbs sampling\n",
      "Iteration 272 of (collapsed) Gibbs sampling\n",
      "Iteration 273 of (collapsed) Gibbs sampling\n",
      "Iteration 274 of (collapsed) Gibbs sampling\n",
      "Iteration 275 of (collapsed) Gibbs sampling\n",
      "Iteration 276 of (collapsed) Gibbs sampling\n",
      "Iteration 277 of (collapsed) Gibbs sampling\n",
      "Iteration 278 of (collapsed) Gibbs sampling\n",
      "Iteration 279 of (collapsed) Gibbs sampling\n",
      "Iteration 280 of (collapsed) Gibbs sampling\n",
      "Iteration 281 of (collapsed) Gibbs sampling\n",
      "Iteration 282 of (collapsed) Gibbs sampling\n",
      "Iteration 283 of (collapsed) Gibbs sampling\n",
      "Iteration 284 of (collapsed) Gibbs sampling\n",
      "Iteration 285 of (collapsed) Gibbs sampling\n",
      "Iteration 286 of (collapsed) Gibbs sampling\n",
      "Iteration 287 of (collapsed) Gibbs sampling\n",
      "Iteration 288 of (collapsed) Gibbs sampling\n",
      "Iteration 289 of (collapsed) Gibbs sampling\n",
      "Iteration 290 of (collapsed) Gibbs sampling\n",
      "Iteration 291 of (collapsed) Gibbs sampling\n",
      "Iteration 292 of (collapsed) Gibbs sampling\n",
      "Iteration 293 of (collapsed) Gibbs sampling\n",
      "Iteration 294 of (collapsed) Gibbs sampling\n",
      "Iteration 295 of (collapsed) Gibbs sampling\n",
      "Iteration 296 of (collapsed) Gibbs sampling\n",
      "Iteration 297 of (collapsed) Gibbs sampling\n",
      "Iteration 298 of (collapsed) Gibbs sampling\n",
      "Iteration 299 of (collapsed) Gibbs sampling\n",
      "Iteration 300 of (collapsed) Gibbs sampling\n",
      "Iteration 301 of (collapsed) Gibbs sampling\n",
      "Iteration 302 of (collapsed) Gibbs sampling\n",
      "Iteration 303 of (collapsed) Gibbs sampling\n",
      "Iteration 304 of (collapsed) Gibbs sampling\n",
      "Iteration 305 of (collapsed) Gibbs sampling\n",
      "Iteration 306 of (collapsed) Gibbs sampling\n",
      "Iteration 307 of (collapsed) Gibbs sampling\n",
      "Iteration 308 of (collapsed) Gibbs sampling\n",
      "Iteration 309 of (collapsed) Gibbs sampling\n",
      "Iteration 310 of (collapsed) Gibbs sampling\n",
      "Iteration 311 of (collapsed) Gibbs sampling\n",
      "Iteration 312 of (collapsed) Gibbs sampling\n",
      "Iteration 313 of (collapsed) Gibbs sampling\n",
      "Iteration 314 of (collapsed) Gibbs sampling\n",
      "Iteration 315 of (collapsed) Gibbs sampling\n",
      "Iteration 316 of (collapsed) Gibbs sampling\n",
      "Iteration 317 of (collapsed) Gibbs sampling\n",
      "Iteration 318 of (collapsed) Gibbs sampling\n",
      "Iteration 319 of (collapsed) Gibbs sampling\n",
      "Iteration 320 of (collapsed) Gibbs sampling\n",
      "Iteration 321 of (collapsed) Gibbs sampling\n",
      "Iteration 322 of (collapsed) Gibbs sampling\n",
      "Iteration 323 of (collapsed) Gibbs sampling\n",
      "Iteration 324 of (collapsed) Gibbs sampling\n",
      "Iteration 325 of (collapsed) Gibbs sampling\n",
      "Iteration 326 of (collapsed) Gibbs sampling\n",
      "Iteration 327 of (collapsed) Gibbs sampling\n",
      "Iteration 328 of (collapsed) Gibbs sampling\n",
      "Iteration 329 of (collapsed) Gibbs sampling\n",
      "Iteration 330 of (collapsed) Gibbs sampling\n",
      "Iteration 331 of (collapsed) Gibbs sampling\n",
      "Iteration 332 of (collapsed) Gibbs sampling\n",
      "Iteration 333 of (collapsed) Gibbs sampling\n",
      "Iteration 334 of (collapsed) Gibbs sampling\n",
      "Iteration 335 of (collapsed) Gibbs sampling\n",
      "Iteration 336 of (collapsed) Gibbs sampling\n",
      "Iteration 337 of (collapsed) Gibbs sampling\n",
      "Iteration 338 of (collapsed) Gibbs sampling\n",
      "Iteration 339 of (collapsed) Gibbs sampling\n",
      "Iteration 340 of (collapsed) Gibbs sampling\n",
      "Iteration 341 of (collapsed) Gibbs sampling\n",
      "Iteration 342 of (collapsed) Gibbs sampling\n",
      "Iteration 343 of (collapsed) Gibbs sampling\n",
      "Iteration 344 of (collapsed) Gibbs sampling\n",
      "Iteration 345 of (collapsed) Gibbs sampling\n",
      "Iteration 346 of (collapsed) Gibbs sampling\n",
      "Iteration 347 of (collapsed) Gibbs sampling\n",
      "Iteration 348 of (collapsed) Gibbs sampling\n",
      "Iteration 349 of (collapsed) Gibbs sampling\n",
      "Iteration 350 of (collapsed) Gibbs sampling\n",
      "Iteration 351 of (collapsed) Gibbs sampling\n",
      "Iteration 352 of (collapsed) Gibbs sampling\n",
      "Iteration 353 of (collapsed) Gibbs sampling\n",
      "Iteration 354 of (collapsed) Gibbs sampling\n",
      "Iteration 355 of (collapsed) Gibbs sampling\n",
      "Iteration 356 of (collapsed) Gibbs sampling\n",
      "Iteration 357 of (collapsed) Gibbs sampling\n",
      "Iteration 358 of (collapsed) Gibbs sampling\n",
      "Iteration 359 of (collapsed) Gibbs sampling\n",
      "Iteration 360 of (collapsed) Gibbs sampling\n",
      "Iteration 361 of (collapsed) Gibbs sampling\n",
      "Iteration 362 of (collapsed) Gibbs sampling\n",
      "Iteration 363 of (collapsed) Gibbs sampling\n",
      "Iteration 364 of (collapsed) Gibbs sampling\n",
      "Iteration 365 of (collapsed) Gibbs sampling\n",
      "Iteration 366 of (collapsed) Gibbs sampling\n",
      "Iteration 367 of (collapsed) Gibbs sampling\n",
      "Iteration 368 of (collapsed) Gibbs sampling\n",
      "Iteration 369 of (collapsed) Gibbs sampling\n",
      "Iteration 370 of (collapsed) Gibbs sampling\n",
      "Iteration 371 of (collapsed) Gibbs sampling\n",
      "Iteration 372 of (collapsed) Gibbs sampling\n",
      "Iteration 373 of (collapsed) Gibbs sampling\n",
      "Iteration 374 of (collapsed) Gibbs sampling\n",
      "Iteration 375 of (collapsed) Gibbs sampling\n",
      "Iteration 376 of (collapsed) Gibbs sampling\n",
      "Iteration 377 of (collapsed) Gibbs sampling\n",
      "Iteration 378 of (collapsed) Gibbs sampling\n",
      "Iteration 379 of (collapsed) Gibbs sampling\n",
      "Iteration 380 of (collapsed) Gibbs sampling\n",
      "Iteration 381 of (collapsed) Gibbs sampling\n",
      "Iteration 382 of (collapsed) Gibbs sampling\n",
      "Iteration 383 of (collapsed) Gibbs sampling\n",
      "Iteration 384 of (collapsed) Gibbs sampling\n",
      "Iteration 385 of (collapsed) Gibbs sampling\n",
      "Iteration 386 of (collapsed) Gibbs sampling\n",
      "Iteration 387 of (collapsed) Gibbs sampling\n",
      "Iteration 388 of (collapsed) Gibbs sampling\n",
      "Iteration 389 of (collapsed) Gibbs sampling\n",
      "Iteration 390 of (collapsed) Gibbs sampling\n",
      "Iteration 391 of (collapsed) Gibbs sampling\n",
      "Iteration 392 of (collapsed) Gibbs sampling\n",
      "Iteration 393 of (collapsed) Gibbs sampling\n",
      "Iteration 394 of (collapsed) Gibbs sampling\n",
      "Iteration 395 of (collapsed) Gibbs sampling\n",
      "Iteration 396 of (collapsed) Gibbs sampling\n",
      "Iteration 397 of (collapsed) Gibbs sampling\n",
      "Iteration 398 of (collapsed) Gibbs sampling\n",
      "Iteration 399 of (collapsed) Gibbs sampling\n",
      "Iteration 400 of (collapsed) Gibbs sampling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 401 of (collapsed) Gibbs sampling\n",
      "Iteration 402 of (collapsed) Gibbs sampling\n",
      "Iteration 403 of (collapsed) Gibbs sampling\n",
      "Iteration 404 of (collapsed) Gibbs sampling\n",
      "Iteration 405 of (collapsed) Gibbs sampling\n",
      "Iteration 406 of (collapsed) Gibbs sampling\n",
      "Iteration 407 of (collapsed) Gibbs sampling\n",
      "Iteration 408 of (collapsed) Gibbs sampling\n",
      "Iteration 409 of (collapsed) Gibbs sampling\n",
      "Iteration 410 of (collapsed) Gibbs sampling\n",
      "Iteration 411 of (collapsed) Gibbs sampling\n",
      "Iteration 412 of (collapsed) Gibbs sampling\n",
      "Iteration 413 of (collapsed) Gibbs sampling\n",
      "Iteration 414 of (collapsed) Gibbs sampling\n",
      "Iteration 415 of (collapsed) Gibbs sampling\n",
      "Iteration 416 of (collapsed) Gibbs sampling\n",
      "Iteration 417 of (collapsed) Gibbs sampling\n",
      "Iteration 418 of (collapsed) Gibbs sampling\n",
      "Iteration 419 of (collapsed) Gibbs sampling\n",
      "Iteration 420 of (collapsed) Gibbs sampling\n",
      "Iteration 421 of (collapsed) Gibbs sampling\n",
      "Iteration 422 of (collapsed) Gibbs sampling\n",
      "Iteration 423 of (collapsed) Gibbs sampling\n",
      "Iteration 424 of (collapsed) Gibbs sampling\n",
      "Iteration 425 of (collapsed) Gibbs sampling\n",
      "Iteration 426 of (collapsed) Gibbs sampling\n",
      "Iteration 427 of (collapsed) Gibbs sampling\n",
      "Iteration 428 of (collapsed) Gibbs sampling\n",
      "Iteration 429 of (collapsed) Gibbs sampling\n",
      "Iteration 430 of (collapsed) Gibbs sampling\n",
      "Iteration 431 of (collapsed) Gibbs sampling\n",
      "Iteration 432 of (collapsed) Gibbs sampling\n",
      "Iteration 433 of (collapsed) Gibbs sampling\n",
      "Iteration 434 of (collapsed) Gibbs sampling\n",
      "Iteration 435 of (collapsed) Gibbs sampling\n",
      "Iteration 436 of (collapsed) Gibbs sampling\n",
      "Iteration 437 of (collapsed) Gibbs sampling\n",
      "Iteration 438 of (collapsed) Gibbs sampling\n",
      "Iteration 439 of (collapsed) Gibbs sampling\n",
      "Iteration 440 of (collapsed) Gibbs sampling\n",
      "Iteration 441 of (collapsed) Gibbs sampling\n",
      "Iteration 442 of (collapsed) Gibbs sampling\n",
      "Iteration 443 of (collapsed) Gibbs sampling\n",
      "Iteration 444 of (collapsed) Gibbs sampling\n",
      "Iteration 445 of (collapsed) Gibbs sampling\n",
      "Iteration 446 of (collapsed) Gibbs sampling\n",
      "Iteration 447 of (collapsed) Gibbs sampling\n",
      "Iteration 448 of (collapsed) Gibbs sampling\n",
      "Iteration 449 of (collapsed) Gibbs sampling\n",
      "Iteration 450 of (collapsed) Gibbs sampling\n",
      "Iteration 451 of (collapsed) Gibbs sampling\n",
      "Iteration 452 of (collapsed) Gibbs sampling\n",
      "Iteration 453 of (collapsed) Gibbs sampling\n",
      "Iteration 454 of (collapsed) Gibbs sampling\n",
      "Iteration 455 of (collapsed) Gibbs sampling\n",
      "Iteration 456 of (collapsed) Gibbs sampling\n",
      "Iteration 457 of (collapsed) Gibbs sampling\n",
      "Iteration 458 of (collapsed) Gibbs sampling\n",
      "Iteration 459 of (collapsed) Gibbs sampling\n",
      "Iteration 460 of (collapsed) Gibbs sampling\n",
      "Iteration 461 of (collapsed) Gibbs sampling\n",
      "Iteration 462 of (collapsed) Gibbs sampling\n",
      "Iteration 463 of (collapsed) Gibbs sampling\n",
      "Iteration 464 of (collapsed) Gibbs sampling\n",
      "Iteration 465 of (collapsed) Gibbs sampling\n",
      "Iteration 466 of (collapsed) Gibbs sampling\n",
      "Iteration 467 of (collapsed) Gibbs sampling\n",
      "Iteration 468 of (collapsed) Gibbs sampling\n",
      "Iteration 469 of (collapsed) Gibbs sampling\n",
      "Iteration 470 of (collapsed) Gibbs sampling\n",
      "Iteration 471 of (collapsed) Gibbs sampling\n",
      "Iteration 472 of (collapsed) Gibbs sampling\n",
      "Iteration 473 of (collapsed) Gibbs sampling\n",
      "Iteration 474 of (collapsed) Gibbs sampling\n",
      "Iteration 475 of (collapsed) Gibbs sampling\n",
      "Iteration 476 of (collapsed) Gibbs sampling\n",
      "Iteration 477 of (collapsed) Gibbs sampling\n",
      "Iteration 478 of (collapsed) Gibbs sampling\n",
      "Iteration 479 of (collapsed) Gibbs sampling\n",
      "Iteration 480 of (collapsed) Gibbs sampling\n",
      "Iteration 481 of (collapsed) Gibbs sampling\n",
      "Iteration 482 of (collapsed) Gibbs sampling\n",
      "Iteration 483 of (collapsed) Gibbs sampling\n",
      "Iteration 484 of (collapsed) Gibbs sampling\n",
      "Iteration 485 of (collapsed) Gibbs sampling\n",
      "Iteration 486 of (collapsed) Gibbs sampling\n",
      "Iteration 487 of (collapsed) Gibbs sampling\n",
      "Iteration 488 of (collapsed) Gibbs sampling\n",
      "Iteration 489 of (collapsed) Gibbs sampling\n",
      "Iteration 490 of (collapsed) Gibbs sampling\n",
      "Iteration 491 of (collapsed) Gibbs sampling\n",
      "Iteration 492 of (collapsed) Gibbs sampling\n",
      "Iteration 493 of (collapsed) Gibbs sampling\n",
      "Iteration 494 of (collapsed) Gibbs sampling\n",
      "Iteration 495 of (collapsed) Gibbs sampling\n",
      "Iteration 496 of (collapsed) Gibbs sampling\n",
      "Iteration 497 of (collapsed) Gibbs sampling\n",
      "Iteration 498 of (collapsed) Gibbs sampling\n",
      "Iteration 499 of (collapsed) Gibbs sampling\n",
      "Iteration 500 of (collapsed) Gibbs sampling\n"
     ]
    }
   ],
   "source": [
    "ldaobj.sample(0,50,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 923.89691508,  880.54024219,  866.29460983,  855.28670368,\n",
       "        851.03009011,  850.92834905,  845.96828092,  845.57143712,\n",
       "        844.06261119,  844.70933996,  843.30708767,  843.52821489,\n",
       "        842.49090964,  840.30760585,  840.7711441 ,  841.34666194,\n",
       "        839.10837937,  839.73479443,  838.55029123,  839.31050467])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldaobj.perplexity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the topic model has improved, and may yet improve more with further sampling, but at this point we will stop to continue with the tutorial.  (In research applications, one would normally apply some convergence criterion to determine the stopping point.)  Ideally we'd like to throw away the initial samples and only keep the last ones.  ldaobj.samples_keep(n) keeps the last n samples of the chain (users can also pass a list of numbers corresponding to the indices they'd like to keep - remember that Python uses 0-indexing).  We will keep the last four samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldaobj.samples_keep(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have only been sampling topic assignments for all the words in the dataset.  What we really care about are the topics and the distribution of topics in each document.  ldaobj has been carrying these around for us while we have been sampling.  ldaobj.tt are the esitmated topics, and ldaobj.dt are the estimated document-topic distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4742, 30, 4)\n",
      "(9488, 30, 4)\n"
     ]
    }
   ],
   "source": [
    "print ldaobj.tt.shape \n",
    "print ldaobj.dt.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated topics are represented by $4742 \\times 30$ matrices whose columns sum to one, one for each sample, while the estimated distributions of topics within each document are represented by $9488 \\times 30$ matrices whose rows sum to one.  To get an idea of the topics that have been estimated, and whether they make sense, ldaobj.topic_content(n) produces topic_description.csv in the working directory.  Its rows contain the first n stems in each topic ranked according their probability, using the final stored sample.  It's a good idea to check the topics are \"reasonable\" before proceeding with any analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldaobj.topic_content(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most economics researchers will probably be most interested initially in the distributions of topics within each document.  To generate these, one should average the matrices in ldaobj.dt.  Here we have only taken four samples for purposes of illustration, but in actual research one should ideally take as many as is computationally feasible.  ldaobj has a convenience method for doing this average, which will both return it as well as, by default, write it to dt.csv in the working directory (to disable printing, pass False to the method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = ldaobj.dt_avg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One might also be interested in the average topics themselves, in which case there is a similar convenience function available that writes tt.csv to the working directory by default.  Each unique stem in the data is associated to a number corresponding to rows of the topic matrices.  Therefore, in most cases one will probably want to print out this key too, available as ldaobj.dict_print."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = ldaobj.tt_avg()\n",
    "ldaobj.dict_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One might also want to replace the speech field in the original dataset with the estimated topics in order to have a ready-to-go dataset for regression or other econometric analysis.  The following code builds this dataset, and also writes it to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop('speech',1)\n",
    "for i in xrange(ldaobj.K): data['T' + str(i)] = dt[:,i]\n",
    "data.to_csv(\"final_output.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If one wishes to analyze some function of the estimated document-topic distributions, this function should be computed for each separate sample and then averaged.  Since the relevant functions are context-specific, topicmodels.LDA does not provide them, but it can be easily extended to accomodate this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying Using Estimated Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After estimating a topic model, one is often interested in estimating the distribution of topics for documents not included in estimation.  In this case, one option is to $\\textit{query}$ those documents by holding fixed the topics estimated from LDA, and only estimating the distribution of topics for the out-of-sample documents.  The topicmodels module also provides a class for such querying, which this section introduces.\n",
    "\n",
    "We will apply querying to the corpus of entire State of the Union Addresses since 1947 (recall that we estimated topics on the level of the paragraph within each speech).  In terms of estimating topics, the paragraph level is preferable to the speech level since individual paragraphs are more likely to be based around a single theme.  But, in terms of econometric work, the entire speech is a more natural unit of analysis.  At the same time, there is no general way of \"adding up\" probability distribution at the paragraph level in order to arrive at a speech-level distribution.  Hence the need for querying, which allows us to estimate the speech-level distributions.  (Extra credit: after the tutorial, estimate LDA on the entire speech level, and judge for yourself how the topics compare to those estimated at the paragraph level).\n",
    "\n",
    "The Query class is initialized in much the same way as LDA, but takes two additional objects: a 3-D array of estimated topics (number of tokens in the estimated topics $\\times$ number of estimated topics $\\times$ number of samples from the estimation); and a dictionary that maps tokens into an index.  We can just pass these directly from ldaobj, which contains data from the above estimated LDA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['speech'] = [' '.join(s) for s in docsobj.stems] # replace the speech field in the original data with its cleaned version from docsobj\n",
    "aggspeeches = data.groupby(['year','president'])['speech'].apply(lambda x: ' '.join(x)) # aggregate up to the speech level\n",
    "aggdocs = topicmodels.RawDocs(aggspeeches) # create new RawDocs object that contains entire speech stems in aggdocs.tokens\n",
    "queryobj = topicmodels.LDA.QueryGibbs(aggdocs.tokens,ldaobj.token_key,ldaobj.tt) # initialize query object with ldaobj attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing, suppose that we instead wanted to query a document whose constitutent parts had not been included in estimation, for example a State of the Union Address from the 1930s.  How to proceed?  First, create a RawDocs object with the text to be queried (recall that RawDocs can take a basic text file, which each new line treated as a separate documents).  Second, perform the same cleaning steps as were done for the documents that went into the estimated model.  However, there is no need to do any stopword removal.  When you initialize a Query object, tokens in the documents to be queried that are not present in the estimated model are automatically stripped out.\n",
    "\n",
    "Since we don't need to estimate topics when querying, we can use far fewer iterations.  Let's start with 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0 queried\n",
      "Sample 1 queried\n",
      "Sample 2 queried\n",
      "Sample 3 queried\n"
     ]
    }
   ],
   "source": [
    "queryobj.query(10) # query our four samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convince yourself that we don't need many iterations, let's look at the perplexity of the data at the entire speech level.  Notice that it is much higher than the perplexity of the data at the paragraph level.  This indicates that the topic model predicts content at the paragraph level much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1231.63687372,  1232.32936859,  1232.94628088,  1233.81706803])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queryobj.perplexity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's triple the number of iterations to 30 and again look at the perplexity.  (Unlike LDA's sampling, each call to query starts sampling from scratch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0 queried\n",
      "Sample 1 queried\n",
      "Sample 2 queried\n",
      "Sample 3 queried\n"
     ]
    }
   ],
   "source": [
    "queryobj.query(30) # query our four samples using more iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1231.56347417,  1232.36687903,  1232.91327431,  1233.75362706])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queryobj.perplexity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note these values are nearly exactly the same as for the 10-iteration querying.\n",
    "\n",
    "Finally, we follow similar steps as for LDA to output the estimated distribution of topics for entire speeches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_query = queryobj.dt_avg()\n",
    "aggdata = pd.DataFrame(dt_query,index=aggspeeches.index,columns=['T' + str(i) for i in xrange(queryobj.K)])\n",
    "aggdata.to_csv(\"final_output_agg.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you can use all of the csv files this tutorial has generated with your statistical software of choice (should this not be Python!) to analyze the topics.  Before finishing, though, we can perform an initial test of whether our output makes sense intuitively.  The following code determines each President's top topics, as measured in terms of deviations from the sample average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_topics(x):\n",
    "\ttop = x.values.argsort()[-5:][::-1]\n",
    "\treturn(pd.Series(top,index=range(1,6)))\n",
    "\n",
    "temp = aggdata.reset_index()\n",
    "ranking = temp.set_index('president')\n",
    "ranking = ranking - ranking.mean()\n",
    "ranking = ranking.groupby(level='president').mean()\n",
    "ranking = ranking.sort_values('year')\n",
    "ranking = ranking.drop('year',1)\n",
    "ranking = ranking.apply(top_topics,axis=1)\n",
    "ranking.to_csv(\"president_top_topics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this particular topic model, for example, George W. Bush's top topic contains words relating to military force, and Obama's employment and economic activity.  The topic model you estimate will of course vary, so I encourage you to open president_top_policy_topics.csv and topic_description.csv to have a look for yourself.  Note too that some topics probably relate to policy, while some others relate to pure rhetoric.  Depending on the nature of the analysis you want to do with the data, it may make sense to restrict attention to some subset of the estimated topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all for now, I hope you enjoyed the tutorial, and begin to use topic modelling in your own work!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
